{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-overview",
   "metadata": {},
   "source": [
    "# DistilBERT Quantization \u2014 Consolidated Notebook\n",
    "\n",
    "This single notebook covers both flows from `nlp_quant_test.ipynb` and `weight_activation_test.ipynb`, using shared utilities in `quant_utils.py`.\n",
    "- Choose quantization mode: dynamic INT8 activations + weights (`int8_dynamic`) or weight-only (`w4`, `w8`).\n",
    "- Evaluate via tokenized pipeline (accuracy + latency) or dataloader (accuracy + per-batch latency).\n",
    "- Optionally fine-tune baseline before quantization.\n",
    "- Optionally save the quantized model for reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from quant_utils import (\n",
    "    load_sst2_dataloaders,\n",
    "    train_sst2_baseline,\n",
    "    quantize_model,\n",
    "    save_quantized_model,\n",
    "    load_base_model,\n",
    "    evaluate_dataloader,\n",
    "    eval_sst2_acc,\n",
    "    bench_latency,\n",
    ")\n",
    "\n",
    "from quant_eval import EvalConfig, evaluate_pair\n",
    "\n",
    "def pick_device(name: str) -> str:\n",
    "    if name == 'cuda' and torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    if name == 'mps' and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    return 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data\n",
    "MODEL_NAME = 'distilbert-sst2-finetuned-128-edited'\n",
    "\n",
    "def _resolve_model_dir() -> str:\n",
    "    candidates = []\n",
    "    cwd = Path.cwd()\n",
    "    candidates.append((cwd / 'models' / MODEL_NAME).resolve())\n",
    "    candidates.append((cwd / 'QAT_pipeline' / 'models' / MODEL_NAME).resolve())\n",
    "    try:\n",
    "        nb_dir = Path(__file__).resolve().parent\n",
    "    except NameError:\n",
    "        nb_dir = Path.cwd()\n",
    "    candidates.append((nb_dir / '..' / 'models' / MODEL_NAME).resolve())\n",
    "    searched = ', '.join(str(p) for p in candidates)\n",
    "    for path in candidates:\n",
    "        if path.exists():\n",
    "            return str(path)\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate '{MODEL_NAME}' in any expected location: {searched}\"\n",
    "    )\n",
    "\n",
    "MODEL_DIR = _resolve_model_dir()\n",
    "DEVICE    = 'cuda'                                # 'cpu' | 'cuda' | 'mps'\n",
    "MAX_LEN   = 128\n",
    "\n",
    "# Quantization\n",
    "MODE       = 'int8_dynamic'  # 'int8_dynamic' | 'w4' | 'w8'\n",
    "GROUP_SIZE = 32              # used for weight-only modes\n",
    "\n",
    "# Evaluation\n",
    "EVAL_MODE        = 'tokenized'  # 'tokenized' | 'dataloader'\n",
    "BATCH_SIZE       = 128           # for dataloader eval\n",
    "MEASURE_BATCHES  = 200           # for dataloader eval\n",
    "WARMUP           = 2             # for dataloader eval\n",
    "BENCH_BATCH      = 32            # for tokenized latency bench\n",
    "BENCH_RUNS       = 50            # for tokenized latency bench\n",
    "BENCH_WARMUP     = 5             # for tokenized latency bench\n",
    "COMPARE_FP32     = True\n",
    "\n",
    "# Optional quick fine-tune (set >0 to enable)\n",
    "TRAIN_EPOCHS = 0\n",
    "LR           = 5e-5\n",
    "\n",
    "# Saving\n",
    "SAVE     = True\n",
    "SAVE_DIR = ''  # default: f\"{MODEL_DIR}-quantized-{MODE}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-header",
   "metadata": {},
   "source": [
    "## Load (or train) baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = pick_device(DEVICE)\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "if TRAIN_EPOCHS and TRAIN_EPOCHS > 0:\n",
    "    model, tok = train_sst2_baseline(\n",
    "        model_name_or_dir=MODEL_DIR,\n",
    "        epochs=TRAIN_EPOCHS,\n",
    "        lr=LR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_len=MAX_LEN,\n",
    "        device=device,\n",
    "    )\n",
    "else:\n",
    "    model, tok = load_base_model(MODEL_DIR, device=device)\n",
    "\n",
    "# Keep original FP32 model intact for comparison\n",
    "model = model.eval()\n",
    "print('Baseline loaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantize-header",
   "metadata": {},
   "source": [
    "## Quantize (torchao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize a deep copy to avoid mutating FP32 baseline\n",
    "qmodel = quantize_model(deepcopy(model), mode=MODE, group_size=GROUP_SIZE)\n",
    "qmodel = qmodel.to(device).eval()\n",
    "print(f'Quantized with MODE={MODE}, group_size={GROUP_SIZE}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-header",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cfg = EvalConfig(\n",
    "    mode=EVAL_MODE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    measure_batches=MEASURE_BATCHES,\n",
    "    warmup=WARMUP,\n",
    "    bench_batch=BENCH_BATCH,\n",
    "    bench_runs=BENCH_RUNS,\n",
    "    bench_warmup=BENCH_WARMUP,\n",
    "    compare_fp32=COMPARE_FP32,\n",
    "    max_len=MAX_LEN,\n",
    ")\n",
    "\n",
    "results = evaluate_pair(\n",
    "    baseline=model,\n",
    "    quantized=qmodel,\n",
    "    tokenizer=tok,\n",
    "    model_dir=MODEL_DIR,\n",
    "    device=device,\n",
    "    config=eval_cfg,\n",
    ")\n",
    "\n",
    "if eval_cfg.mode == 'dataloader':\n",
    "    print('Quantized:', results['quantized'])\n",
    "    if COMPARE_FP32 and 'fp32' in results:\n",
    "        print('FP32:', results['fp32'])\n",
    "else:\n",
    "    q = results['quantized']\n",
    "    print(f\"Q({MODE}): acc={q['acc']:.4f},  avg_batch_latency={q['avg_batch_latency_ms']:.1f} ms\")\n",
    "    if COMPARE_FP32 and 'fp32' in results:\n",
    "        fp = results['fp32']\n",
    "        print(f\"FP32 : acc={fp['acc']:.4f},  avg_batch_latency={fp['avg_batch_latency_ms']:.1f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save quantized model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    out_dir = SAVE_DIR or (MODEL_DIR.rstrip('/') + f'-quantized-{MODE}')\n",
    "    save_quantized_model(qmodel, tok, out_dir)\n",
    "    print('Saved quantized model to:', out_dir)\n",
    "else:\n",
    "    print('Skipping save (set SAVE=True to enable).')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qat_pipline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}