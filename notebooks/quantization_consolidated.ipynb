{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-overview",
   "metadata": {},
   "source": [
    "# DistilBERT Quantization â€” Consolidated Notebook\n",
    "\n",
    "This single notebook covers both flows from `nlp_quant_test.ipynb` and `weight_activation_test.ipynb`, using shared utilities in `quant_utils.py`.\n",
    "- Choose quantization mode: dynamic INT8 activations + weights (`int8_dynamic`) or weight-only (`w4`, `w8`).\n",
    "- Evaluate via tokenized pipeline (accuracy + latency) or dataloader (accuracy + per-batch latency).\n",
    "- Optionally fine-tune baseline before quantization.\n",
    "- Optionally save the quantized model for reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0917 11:23:19.293000 31192 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "from quant_utils import (\n",
    "    load_sst2_dataloaders,\n",
    "    train_sst2_baseline,\n",
    "    quantize_model,\n",
    "    save_quantized_model,\n",
    "    load_base_model,\n",
    "    evaluate_dataloader,\n",
    "    eval_sst2_acc,\n",
    "    bench_latency,\n",
    ")\n",
    "\n",
    "def pick_device(name: str) -> str:\n",
    "    if name == 'cuda' and torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    if name == 'mps' and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    return 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data\n",
    "MODEL_DIR = './distilbert-sst2-finetuned-128'   # HF dir or model name\n",
    "DEVICE    = 'cpu'                                # 'cpu' | 'cuda' | 'mps'\n",
    "MAX_LEN   = 128\n",
    "\n",
    "# Quantization\n",
    "MODE       = 'int8_dynamic'  # 'int8_dynamic' | 'w4' | 'w8'\n",
    "GROUP_SIZE = 32              # used for weight-only modes\n",
    "\n",
    "# Evaluation\n",
    "EVAL_MODE        = 'tokenized'  # 'tokenized' | 'dataloader'\n",
    "BATCH_SIZE       = 128           # for dataloader eval\n",
    "MEASURE_BATCHES  = 200           # for dataloader eval\n",
    "WARMUP           = 2             # for dataloader eval\n",
    "BENCH_BATCH      = 32            # for tokenized latency bench\n",
    "BENCH_RUNS       = 50            # for tokenized latency bench\n",
    "BENCH_WARMUP     = 5             # for tokenized latency bench\n",
    "COMPARE_FP32     = True\n",
    "\n",
    "# Optional quick fine-tune (set >0 to enable)\n",
    "TRAIN_EPOCHS = 0\n",
    "LR           = 5e-5\n",
    "\n",
    "# Saving\n",
    "SAVE     = True\n",
    "SAVE_DIR = ''  # default: f\"{MODEL_DIR}-quantized-{MODE}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-header",
   "metadata": {},
   "source": [
    "## Load (or train) baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Baseline loaded.\n"
     ]
    }
   ],
   "source": [
    "device = pick_device(DEVICE)\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "if TRAIN_EPOCHS and TRAIN_EPOCHS > 0:\n",
    "    model, tok = train_sst2_baseline(\n",
    "        model_name_or_dir=MODEL_DIR,\n",
    "        epochs=TRAIN_EPOCHS,\n",
    "        lr=LR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_len=MAX_LEN,\n",
    "        device=device,\n",
    "    )\n",
    "else:\n",
    "    model, tok = load_base_model(MODEL_DIR, device=device)\n",
    "\n",
    "# Keep original FP32 model intact for comparison\n",
    "model = model.eval()\n",
    "print('Baseline loaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantize-header",
   "metadata": {},
   "source": [
    "## Quantize (torchao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quantize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized with MODE=int8_dynamic, group_size=32\n"
     ]
    }
   ],
   "source": [
    "# Quantize a deep copy to avoid mutating FP32 baseline\n",
    "qmodel = quantize_model(deepcopy(model), mode=MODE, group_size=GROUP_SIZE)\n",
    "qmodel = qmodel.to(device).eval()\n",
    "print(f'Quantized with MODE={MODE}, group_size={GROUP_SIZE}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-header",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "evaluate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(int8_dynamic): acc=0.9117,  avg_batch_latency=509.9 ms\n",
      "FP32 : acc=0.9140,  avg_batch_latency=18.8 ms\n"
     ]
    }
   ],
   "source": [
    "if EVAL_MODE == 'dataloader':\n",
    "    _, _, val_loader = load_sst2_dataloaders(MODEL_DIR, max_len=MAX_LEN, batch_size=BATCH_SIZE)\n",
    "    qmetrics = evaluate_dataloader(qmodel, val_loader, device=device, warmup=WARMUP, measure_batches=MEASURE_BATCHES)\n",
    "    print('Quantized:', qmetrics)\n",
    "    if COMPARE_FP32:\n",
    "        fp32_metrics = evaluate_dataloader(model, val_loader, device=device, warmup=WARMUP, measure_batches=MEASURE_BATCHES)\n",
    "        print('FP32:', fp32_metrics)\n",
    "else:\n",
    "    # Tokenized pipeline: accuracy on SST-2 + simple latency bench\n",
    "    acc_q = eval_sst2_acc(qmodel, tok, device=device, split='validation', bs=BENCH_BATCH, max_len=MAX_LEN)\n",
    "    lat_q = bench_latency(qmodel, tok, device=device, bs=BENCH_BATCH, runs=BENCH_RUNS, warmup=BENCH_WARMUP, max_len=MAX_LEN)\n",
    "    print(f\"Q({MODE}): acc={acc_q:.4f},  avg_batch_latency={lat_q*1000:.1f} ms\")\n",
    "    if COMPARE_FP32:\n",
    "        acc_fp = eval_sst2_acc(model, tok, device=device, split='validation', bs=BENCH_BATCH, max_len=MAX_LEN)\n",
    "        lat_fp = bench_latency(model, tok, device=device, bs=BENCH_BATCH, runs=BENCH_RUNS, warmup=BENCH_WARMUP, max_len=MAX_LEN)\n",
    "        print(f\"FP32 : acc={acc_fp:.4f},  avg_batch_latency={lat_fp*1000:.1f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save quantized model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved quantized model to: ./distilbert-sst2-finetuned-128-quantized-int8_dynamic\n"
     ]
    }
   ],
   "source": [
    "if SAVE:\n",
    "    out_dir = SAVE_DIR or (MODEL_DIR.rstrip('/') + f'-quantized-{MODE}')\n",
    "    save_quantized_model(qmodel, tok, out_dir)\n",
    "    print('Saved quantized model to:', out_dir)\n",
    "else:\n",
    "    print('Skipping save (set SAVE=True to enable).')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qat_pipline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
