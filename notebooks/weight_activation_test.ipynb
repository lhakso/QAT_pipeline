{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684c5f84-858c-4553-b5fe-f8ed7bc1a1be",
   "metadata": {},
   "source": [
    "# Quantize Model & Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d365b10e-2a84-4afc-bbae-468af5951015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODE = int8_dynamic ===\n",
      "FP32 : acc=0.9140,  avg_batch_latency=18.4 ms\n",
      "Q(int8_dynamic): acc=0.9117,  avg_batch_latency=904.1 ms\n"
     ]
    }
   ],
   "source": [
    "# ===== Torch-only, torchao-based quant baselines for DistilBERT (SST-2) =====\n",
    "import time, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 1) pick your baseline:\n",
    "#    \"int8_dynamic\" = W8A8 dynamic activations (recommended, robust)\n",
    "#    \"w4\"           = INT4 weight-only (W4), activations stay float\n",
    "MODE = \"int8_dynamic\"   # or \"w4\"\n",
    "\n",
    "MODEL_DIR = \"./distilbert-sst2-finetuned-128\"\n",
    "MAX_LEN   = 128\n",
    "DEVICE    = \"cpu\"        # keep it torch-only; you can switch to \"cuda\" if available\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR).eval().to(DEVICE)\n",
    "\n",
    "# 2) quantize with torchao.quantization.quantize_\n",
    "from torchao.quantization import quantize_, Int8DynamicActivationInt8WeightConfig, Int4WeightOnlyConfig\n",
    "\n",
    "if MODE == \"int8_dynamic\":\n",
    "    # dynamic per-token activations (int8) + per-channel int8 weights on Linear layers\n",
    "    quantize_(model, Int8DynamicActivationInt8WeightConfig())\n",
    "elif MODE == \"w4\":\n",
    "    # weight-only int4 per-group quant on Linear layers (activations stay float)\n",
    "    # group_size=32 is a common default; adjust if you want\n",
    "    quantize_(model, Int4WeightOnlyConfig(group_size=32))\n",
    "else:\n",
    "    raise ValueError(\"MODE must be 'int8_dynamic' or 'w4'\")\n",
    "\n",
    "# 3) helper\n",
    "@torch.no_grad()\n",
    "def predict_logits(m, texts, max_len=128):\n",
    "    enc = tok(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).to(DEVICE)\n",
    "    return m(**enc).logits.detach().cpu().numpy()\n",
    "\n",
    "def eval_sst2_acc(m, split=\"validation\", bs=32):\n",
    "    ds = load_dataset(\"glue\", \"sst2\", split=split)\n",
    "    labels = np.array(ds[\"label\"], dtype=np.int64)\n",
    "    preds = []\n",
    "    for i in range(0, len(ds), bs):\n",
    "        logits = predict_logits(m, ds[\"sentence\"][i:i+bs], MAX_LEN)\n",
    "        preds.append(np.argmax(logits, axis=-1))\n",
    "    preds = np.concatenate(preds)\n",
    "    return float((preds == labels[:len(preds)]).mean())\n",
    "\n",
    "def bench_latency(m, text=\"this movie was great!\", bs=32, runs=50, warmup=5):\n",
    "    batch = [text] * bs\n",
    "    _ = predict_logits(m, batch, MAX_LEN)  # warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = predict_logits(m, batch, MAX_LEN)\n",
    "    t0 = time.time()\n",
    "    for _ in range(runs):\n",
    "        _ = predict_logits(m, batch, MAX_LEN)\n",
    "    return (time.time() - t0) / runs\n",
    "\n",
    "# 4) (optional) compare against FP32 quickly\n",
    "base = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR).eval().to(DEVICE)\n",
    "\n",
    "acc_fp  = eval_sst2_acc(base)\n",
    "acc_q   = eval_sst2_acc(model)\n",
    "lat_fp  = bench_latency(base)\n",
    "lat_q   = bench_latency(model)\n",
    "\n",
    "print(f\"\\n=== MODE = {MODE} ===\")\n",
    "print(f\"FP32 : acc={acc_fp:.4f},  avg_batch_latency={lat_fp*1000:.1f} ms\")\n",
    "print(f\"Q({MODE}): acc={acc_q:.4f},  avg_batch_latency={lat_q*1000:.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0ed1b25-3cac-41a6-a9c4-d8ab884f8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, dataloader, device=\"cpu\", warmup=2, measure_batches=200):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    times = []\n",
    "\n",
    "    # warmup passes (not timed)\n",
    "    it = iter(dataloader)\n",
    "    for _ in range(min(warmup, len(dataloader))):\n",
    "        batch = next(it, None)\n",
    "        if batch is None: break\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        _ = model(**batch).logits\n",
    "\n",
    "    # timed passes\n",
    "    counted = 0\n",
    "    for batch in dataloader:\n",
    "        if counted >= measure_batches: break\n",
    "        labels = batch[\"labels\"]\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        logits = model(**batch).logits\n",
    "        dt = (time.perf_counter() - t0) * 1000.0  # ms\n",
    "        times.append(dt)\n",
    "\n",
    "        preds = logits.argmax(dim=-1).cpu()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        counted += 1\n",
    "\n",
    "    acc = 100.0 * correct / total if total else 0.0\n",
    "    mean_ms = sum(times) / len(times) if times else float(\"nan\")\n",
    "    return {\"acc\": acc, \"mean_ms_per_batch\": mean_ms, \"batches_measured\": counted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d9781-89ef-4ddd-830c-f6954032d07e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(qat)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
